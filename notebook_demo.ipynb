{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot vegetation coverage prediction using PointNet-based model\n",
    "## Dataset\n",
    "\n",
    "The working dataset contains ~200 circulat plots of 10m radius.\n",
    "Each point has 9 features : XYZ, RGB, NIR, intensity, return number\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"exemples_images/POINT_OBS9_cl.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"exemples_images/POINT_OBS10_cl.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first start by importing and installing the necessary libraires:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import functools\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.stats import gamma\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchnet as tnt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import gc\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "import torch.nn as nn\n",
    "from laspy.file import File\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.special import digamma, polygamma\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "np.random.seed(42)\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We set model parameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='model')\n",
    "\n",
    "\n",
    "# System Parameters\n",
    "parser.add_argument('--path', default=\"/home/ign.fr/ekalinicheva/DATASET_regression/\", type=str,\n",
    "                    help=\"Main folder directory\")\n",
    "parser.add_argument('--gt_file', default=\"resultats_placettes_combo.csv\", type=str, help=\"Name of GT *.cvs file\")\n",
    "parser.add_argument('--plot_folder_name', default=\"placettes_combo\", type=str, help=\"Name of GT *.csv file\")\n",
    "parser.add_argument('--cuda', default=1, type=int, help=\"Whether we use cuda (1) or not (0)\")\n",
    "parser.add_argument('--folds', default=5, type=int, help=\"Number of folds for cross validation model training\")\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--n_class', default=4, type=int,\n",
    "                    help=\"Size of the model output vector. In our case 4 - different vegetation coverage types\")\n",
    "parser.add_argument('--input_feats', default='xyzrgbnir', type=str,\n",
    "                    help=\"Point features that we keep. in this code, we keep them all. permuting those letters will break everything. To be modified\")\n",
    "parser.add_argument('--subsample_size', default=4096, type=int, help=\"Subsample cloud size\")\n",
    "parser.add_argument('--diam_pix', default=32, type=int,\n",
    "                    help=\"Size of the output stratum raster (its diameter in pixels)\")\n",
    "parser.add_argument('--m', default=1, type=float,\n",
    "                    help=\"Loss regularization. The weight of the negative loglikelihood loss in the total loss\")\n",
    "parser.add_argument('--norm_ground', default=False, type=bool,\n",
    "                    help=\"Whether we normalize low vegetation and bare soil values, so LV+BS=1 (True) or we keep unmodified LV value (False) (recommended)\")\n",
    "parser.add_argument('--adm', default=False, type=bool, help=\"Whether we compute admissibility or not\")\n",
    "parser.add_argument('--nb_stratum', default=3, type=int,\n",
    "                    help=\"[2, 3] Number of vegetation stratum that we compute 2 - ground level + medium level; 3 - ground level + medium level + high level\")\n",
    "parser.add_argument('--ECM_ite_max', default=5, type=int, help='Max number of EVM iteration')\n",
    "parser.add_argument('--NR_ite_max', default=10, type=int, help='Max number of Netwon-Rachson iteration')\n",
    "\n",
    "# Network Parameters\n",
    "parser.add_argument('--MLP_1', default=[32, 32], type=list,\n",
    "                    help=\"Parameters of the 1st MLP block (output size of each layer). See PointNet article\")\n",
    "parser.add_argument('--MLP_2', default=[64, 128], type=list,\n",
    "                    help=\"Parameters of the 2nd MLP block (output size of each layer). See PointNet article\")\n",
    "parser.add_argument('--MLP_3', default=[64, 32], type=list,\n",
    "                    help=\"Parameters of the 3rd MLP block (output size of each layer). See PointNet article\")\n",
    "parser.add_argument('--drop', default=0.4, type=float, help=\"Probability value of the DropOut layer of the model\")\n",
    "parser.add_argument('--soft', default=True, type=bool,\n",
    "                    help=\"Whether we use softmax layer for the model output (True) of sigmoid (False)\")\n",
    "\n",
    "# Optimization Parameters\n",
    "parser.add_argument('--wd', default=0.001, type=float, help=\"Weight decay for the optimizer\")\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help=\"Learning rate\")\n",
    "parser.add_argument('--step_size', default=50, type=int,\n",
    "                    help=\"After this number of steps we decrease learning rate. (Period of learning rate decay)\")\n",
    "parser.add_argument('--lr_decay', default=0.1, type=float,\n",
    "                    help=\"We multiply learning rate by this value after certain number of steps (see --step_size). (Multiplicative factor of learning rate decay)\")\n",
    "parser.add_argument('--n_epoch', default=150, type=int, help=\"Number of training epochs\")\n",
    "parser.add_argument('--n_epoch_test', default=5, type=int, help=\"We evaluate every -th epoch\")\n",
    "parser.add_argument('--batch_size', default=20, type=int, help=\"Size of the training batch\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "assert (args.nb_stratum in [2, 3]), \"Number of stratum should be 2 or 3!\"\n",
    "assert (args.lr_decay < 1), \"Learning rate decrease should be smaller than 1, as learning rate should decrease\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print stats to file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def print_stats(stats_file, text, print_to_console=True):\n",
    "    with open(stats_file, 'a') as f:\n",
    "        if isinstance(text, list):\n",
    "            for t in text:\n",
    "                f.write(t + \"\\n\")\n",
    "                if print_to_console:\n",
    "                    print(t)\n",
    "        else:\n",
    "            f.write(text + \"\\n\")\n",
    "            if print_to_console:\n",
    "                print(text)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to create a new folder if does not exists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define some paths to files and folders and check starting time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:57:40\n",
      "Results folder:  /home/ign.fr/ekalinicheva/DATASET_regression/RESULTS_3_stratum/only_stratum/2021-05-06_185740/\n"
     ]
    }
   ],
   "source": [
    "las_folder = os.path.join(args.path, args.plot_folder_name) # folder with las files\n",
    "\n",
    "# We keep track of time and stats\n",
    "start_time = time.time()\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(start_time)))\n",
    "run_name = str(time.strftime(\"%Y-%m-%d_%H%M%S\"))\n",
    "\n",
    "# We write results to different folders depending on the chosen parameters\n",
    "if args.nb_stratum == 2:\n",
    "    results_path = stats_path = os.path.join(args.path, \"RESULTS/\")\n",
    "else:\n",
    "    results_path = stats_path = os.path.join(args.path,\"RESULTS_3_stratum/\")\n",
    "\n",
    "if args.adm:\n",
    "    results_path = os.path.join(results_path, \"admissibility/\")\n",
    "else:\n",
    "    results_path = os.path.join(results_path, \"only_stratum/\")\n",
    "\n",
    "stats_path = os.path.join(results_path, run_name) + \"/\"\n",
    "print(\"Results folder: \", stats_path)\n",
    "stats_file = os.path.join(stats_path, \"stats.txt\")\n",
    "create_dir(stats_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We open las files with plots as numpy arrays:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def open_las(las_folder):\n",
    "    # We open las files and create a training dataset\n",
    "    dataset = {}  # dict to store numpy array with each plot separately\n",
    "    mean_dataset = {}  # we keep track of plots means to reverse the normalisation in the future\n",
    "\n",
    "    # We iterate through las files and transform them to np array\n",
    "    las_files = os.listdir(las_folder)\n",
    "    all_points = np.empty((0, 9))\n",
    "    for las_file in las_files:\n",
    "        las = File(os.path.join(las_folder, las_file), mode='r')\n",
    "        x_las = las.X\n",
    "        y_las = las.Y\n",
    "        z_las = las.Z\n",
    "        r = las.Red\n",
    "        g = las.Green\n",
    "        b = las.Blue\n",
    "        nir = las.nir\n",
    "        intensity = las.intensity\n",
    "        nbr_returns = las.return_num\n",
    "        points_placette = np.asarray([x_las / 100, y_las / 100, z_las / 100, r, g, b, nir, intensity,\n",
    "                                      nbr_returns]).T  # we divide by 100 as all the values in las are in cm\n",
    "\n",
    "        # There is a file with 2 points 60m above others (maybe birds), we delete these points\n",
    "        if las_file == \"Releve_Lidar_F70.las\":\n",
    "            points_placette = points_placette[points_placette[:, 2] < 640]\n",
    "        # We do the same for the intensity\n",
    "        if las_file == \"POINT_OBS8.las\":\n",
    "            points_placette = points_placette[points_placette[:, -2] < 32768]\n",
    "        if las_file == \"Releve_Lidar_F39.las\":\n",
    "            points_placette = points_placette[points_placette[:, -2] < 20000]\n",
    "\n",
    "        # We directly substract z_min at local level\n",
    "        xyz = points_placette[:, :3]\n",
    "        knn = NearestNeighbors(500, algorithm='kd_tree').fit(xyz[:, :2])\n",
    "        _, neigh = knn.radius_neighbors(xyz[:, :2], 0.5)\n",
    "        z = xyz[:, 2]\n",
    "        zmin_neigh = []\n",
    "        for n in range(len(z)):\n",
    "            zmin_neigh.append(np.min(z[neigh[n]]))\n",
    "        points_placette[:, 2] = points_placette[:, 2] - zmin_neigh\n",
    "\n",
    "        all_points = np.append(all_points, points_placette, axis=0)\n",
    "        dataset[os.path.splitext(las_file)[0]] = points_placette\n",
    "        mean_dataset[os.path.splitext(las_file)[0]] = [np.mean(x_las) / 100, np.mean(y_las) / 100]\n",
    "\n",
    "    return all_points, dataset, mean_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in memory\n",
      "Our dataset contains 209 plots!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data in memory\")\n",
    "all_points, dataset, mean_dataset = open_las(las_folder)\n",
    "print(\"Our dataset contains \" + str(len(dataset)) + \" plots!\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extract last parameters and write all of them to stats file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ECM_ite_max=5, MLP_1=[32, 32], MLP_2=[64, 128], MLP_3=[64, 32], NR_ite_max=10, adm=False, batch_size=20, cuda=1, diam_pix=32, drop=0.4, folds=5, gt_file='resultats_placettes_combo.csv', input_feats='xyzrgbnir', lr=0.001, lr_decay=0.1, m=1, n_class=4, n_epoch=1, n_epoch_test=1, n_input_feats=9, nb_stratum=3, norm_ground=False, path='/home/ign.fr/ekalinicheva/DATASET_regression/', plot_folder_name='placettes_combo', soft=True, step_size=50, subsample_size=4096, wd=0.001, z_max=24.140000000000043)\n"
     ]
    }
   ],
   "source": [
    "z_all = all_points[:, 2]\n",
    "args.z_max = np.max(z_all)   # maximum z value for data normalization, obtained from the normalized dataset analysis\n",
    "args.n_input_feats = len(args.input_feats)  # number of input features\n",
    "print_stats(stats_file, str(args), print_to_console=True)  # save all the args parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute parameters of gamma distributions for two stratum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def get_gamma_parameters(all_z, args):\n",
    "    all_z = all_z + 1e-2\n",
    "    #initialization\n",
    "    shape = np.array([0.2, 1.8])  # scale gamma parameters\n",
    "    scale = np.array([0.3, 4]) #shape gamma parameters\n",
    "    pi = np.array([0.5, 5]) #bernoulli parameter\n",
    "    def view_distribution():\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        x = np.linspace(0,10, 100)\n",
    "        plt.hist(all_z, bins=100, range=(0, 10), density=True)\n",
    "        plt.plot(x, pi[0] * gamma.pdf(x, shape[0], 0, scale[0]), 'r-', lw=1, label='gamma1')\n",
    "        plt.plot(x, pi[1] * gamma.pdf(x, shape[1], 0, scale[1]), 'k-', lw=1, label='gamma2')\n",
    "        plt.tight_layout()\n",
    "        axes = plt.gca()\n",
    "        axes.set_ylim([0,0.5])\n",
    "        plt.show(block=True)\n",
    "    def E_step():\n",
    "        expected_values = np.vstack((gamma.pdf(all_z, shape[0], 0, scale[0]),gamma.pdf(all_z, shape[1], 0, scale[1])))\n",
    "        expected_values = expected_values * pi[:, None]\n",
    "        return expected_values/expected_values.sum(0)\n",
    "    def inner_optim(expected_values):\n",
    "        #find simultaneously the CM values for shape defined as poles obj, with with newton-raphson\n",
    "        x = shape\n",
    "        def obj(x):\n",
    "            #the function to minimize\n",
    "            return (expected_values * (np.log(all_z[None,:]) - np.log(scale)[:,None] - digamma(x)[:,None])).mean(1)\n",
    "        def derivative(x):\n",
    "            #its derivative\n",
    "            return (expected_values * (- polygamma(1, x)[:, None])).mean(1)\n",
    "        for sub_ite in range(args.NR_ite_max):\n",
    "            print(\"    NR it %d - obj = %3.3f %3.3f\" % (sub_ite, *obj(x)))\n",
    "            if (np.abs(obj(x))<1e-3).all():\n",
    "                print(\"Newton Rachson terminated\")\n",
    "                break\n",
    "            x = x - obj(x) / derivative(x) #one NR iteration\n",
    "        return x\n",
    "    def CM1(expected_values):\n",
    "        #first CM-step\n",
    "        pi = expected_values.mean(1)\n",
    "        scale = inner_optim(expected_values)\n",
    "        return pi, scale\n",
    "    def CM2(expected_values):\n",
    "        #second CM-step\n",
    "        num = (all_z[None,:] * expected_values).mean(1)\n",
    "        denom = scale * expected_values.mean(1)\n",
    "        return num/denom\n",
    "    def log_likelihood():\n",
    "        expected_values = np.vstack((gamma.pdf(all_z, shape[0], 0, scale[0]), gamma.pdf(all_z, shape[1], 0, scale[1])))\n",
    "        expected_values = expected_values * pi[:, None]\n",
    "        return -np.log(expected_values.sum(0)).mean()\n",
    "    #---main loop---\n",
    "    print(\"Likelihood at init: %2.3f\" % (log_likelihood()))\n",
    "    for ite in range(args.ECM_ite_max):\n",
    "        expected_values = E_step()\n",
    "        pi, scale = CM1(expected_values)\n",
    "        shape = CM2(expected_values)\n",
    "        print(\"Likelihood at ite %d: %2.3f\" % (ite, log_likelihood()))\n",
    "\n",
    "    # We plot the distributions\n",
    "    x = np.linspace(0, 10, 101)[1:]\n",
    "    histo = plt.hist(all_z, bins=100, range=(0, 10), density=True)\n",
    "    bins = histo[1][1:]\n",
    "    pdf =  histo[0]\n",
    "    y1 = pi[0] * gamma.pdf(x, shape[0], 0, scale[0])\n",
    "    y2 = pi[1] * gamma.pdf(x, shape[1], 0, scale[1])\n",
    "    # np.savetxt(\"ECM.csv\",  np.vstack((bins, pdf, y1, y2)).transpose(), delimiter=\",\")\n",
    "    view_distribution()\n",
    "\n",
    "    params = {'phi': pi[0], 'a_g': shape[0], 'a_v': shape[1],\n",
    "              'loc_g': 0, 'loc_v': 0, 'scale_g': scale[0],\n",
    "              'scale_v': scale[1]}\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gamma_file = os.path.join(stats_path, \"gamma.pkl\")\n",
    "if not os.path.isfile(gamma_file):\n",
    "    print(\"Computing gamma mixture (should only happen once)\")\n",
    "    params = get_gamma_parameters(z_all, args)\n",
    "    with open(gamma_file, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "else:\n",
    "    print(\"Found precomputed Gamma parameters\")\n",
    "    with open(gamma_file, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "print_stats(stats_file, str(params), print_to_console=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gamma mixture (should only happen once)\n",
      "Likelihood at init: 0.425\n",
      "    NR it 0 - obj = 1.785 -0.284\n",
      "    NR it 1 - obj = 0.703 0.119\n",
      "    NR it 2 - obj = 0.194 0.011\n",
      "    NR it 3 - obj = 0.023 0.000\n",
      "    NR it 4 - obj = 0.000 0.000\n",
      "Newton Rachson terminated\n",
      "Likelihood at ite 0: 1.653\n",
      "    NR it 0 - obj = 3.083 0.038\n",
      "    NR it 1 - obj = 1.282 0.002\n",
      "    NR it 2 - obj = 0.412 0.000\n",
      "    NR it 3 - obj = 0.070 0.000\n",
      "    NR it 4 - obj = 0.003 0.000\n",
      "    NR it 5 - obj = 0.000 -0.000\n",
      "Newton Rachson terminated\n",
      "Likelihood at ite 1: 1.423\n",
      "    NR it 0 - obj = 1.225 0.030\n",
      "    NR it 1 - obj = 0.400 0.001\n",
      "    NR it 2 - obj = 0.071 0.000\n",
      "    NR it 3 - obj = 0.003 0.000\n",
      "    NR it 4 - obj = 0.000 0.000\n",
      "Newton Rachson terminated\n",
      "Likelihood at ite 2: 1.446\n",
      "    NR it 0 - obj = 1.639 0.009\n",
      "    NR it 1 - obj = 0.584 0.000\n",
      "    NR it 2 - obj = 0.128 0.000\n",
      "    NR it 3 - obj = 0.009 0.000\n",
      "    NR it 4 - obj = 0.000 0.000\n",
      "Newton Rachson terminated\n",
      "Likelihood at ite 3: 1.410\n",
      "    NR it 0 - obj = 1.155 0.030\n",
      "    NR it 1 - obj = 0.364 0.001\n",
      "    NR it 2 - obj = 0.059 0.000\n",
      "    NR it 3 - obj = 0.002 0.000\n",
      "    NR it 4 - obj = 0.000 0.000\n",
      "Newton Rachson terminated\n",
      "Likelihood at ite 4: 1.413\n",
      "{'phi': 0.5454230322404273, 'a_g': 0.2358482257843279, 'a_v': 3.647051411164164, 'loc_g': 0, 'loc_v': 0, 'scale_g': 0.4916964034514489, 'scale_v': 1.4920331793959078}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datalaoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def augment(cloud_data):\n",
    "    \"\"\"augmentation function\n",
    "    Does random rotation around z axis and adds Gaussian noise to all the features, except z and return number\n",
    "    \"\"\"\n",
    "    #random rotation around the Z axis\n",
    "    #angle = random angle 0..2pi\n",
    "    angle = np.radians(np.random.choice(360, 1)[0])\n",
    "    c, s = np.cos(angle), np.sin(angle)\n",
    "    M = np.array(((c, -s), (s, c))) #rotation matrix around axis z with angle \"angle\"\n",
    "    cloud_data[:2] = np.dot(cloud_data[:2].T, M).T #perform the rotation efficiently\n",
    "\n",
    "\n",
    "    #random gaussian noise everywhere except z and return number\n",
    "    sigma, clip = 0.01, 0.03\n",
    "    cloud_data[:2] = cloud_data[:2] + np.clip(sigma*np.random.randn(cloud_data[:2].shape[0], cloud_data[:2].shape[1]), a_min=-clip, a_max=clip).astype(np.float32)\n",
    "    cloud_data[3:8] = cloud_data[3:8] + np.clip(sigma*np.random.randn(cloud_data[3:8].shape[0], cloud_data[3:8].shape[1]), a_min=-clip, a_max=clip).astype(np.float32)\n",
    "    return cloud_data\n",
    "\n",
    "\n",
    "def cloud_loader(plot_id, dataset, df_gt, train, args):\n",
    "    \"\"\"\n",
    "    load a plot and returns points features (normalized xyz + features) and\n",
    "    ground truth\n",
    "    INPUT:\n",
    "    tile_name = string, name of the tile\n",
    "    train = int, train = 1 iff in the train set\n",
    "    OUTPUT\n",
    "    cloud_data, [n x 4] float Tensor containing points coordinates and intensity\n",
    "    labels, [n] long int Tensor, containing the points semantic labels\n",
    "    \"\"\"\n",
    "    cloud_data = np.array(dataset[plot_id]).transpose()\n",
    "    gt = df_gt[df_gt['Name']==plot_id][['COUV_BASSE', 'COUV_SOL', 'COUV_INTER', 'COUV_HAUTE', 'ADM']].values/100\n",
    "    # gt = np.asarray([np.append(gt, [1 - gt[:, 2]])])\n",
    "\n",
    "    xmean, ymean = np.mean(cloud_data[0:2], axis=1)\n",
    "\n",
    "    #normalizing data\n",
    "    # Z data was already partially normalized during loading\n",
    "    cloud_data[0] = (cloud_data[0] - xmean)/10 #x\n",
    "    cloud_data[1] = (cloud_data[1] - ymean)/10 #y\n",
    "    cloud_data[2] = (cloud_data[2])/args.z_max #z\n",
    "\n",
    "\n",
    "    colors_max = 65536\n",
    "    cloud_data[3:7] = cloud_data[3:7]/colors_max\n",
    "    int_max = 32768\n",
    "    cloud_data[7] = cloud_data[7] / int_max\n",
    "    cloud_data[8] = (cloud_data[8] - 1)/(7-1)\n",
    "\n",
    "    if train:\n",
    "      cloud_data = augment(cloud_data)\n",
    "\n",
    "    cloud_data = torch.from_numpy(cloud_data)\n",
    "    gt = torch.from_numpy(gt).float()\n",
    "    return cloud_data, gt\n",
    "\n",
    "\n",
    "def cloud_collate(batch):\n",
    "    \"\"\" Collates a list of dataset samples into a batch list for clouds\n",
    "    and a single array for labels\n",
    "    This function is necessary to implement because the clouds have different sizes (unlike for images)\n",
    "    \"\"\"\n",
    "    clouds, labels = list(zip(*batch))\n",
    "    labels = torch.cat(labels, 0)\n",
    "    return clouds, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions to create final images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 25\n",
    "\n",
    "\n",
    "def visualize_article(image_soil, image_med_veg, image_high_veg, cloud, pl_id, stats_path, args, txt=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "\n",
    "    # Original point data\n",
    "    ax1 = fig.add_subplot(gs[:, 0:2], projection='3d')\n",
    "    colors_ = cloud[3:6].numpy().transpose()\n",
    "    ax1.scatter(cloud[0], cloud[1], cloud[2]*args.z_max, c=colors_, vmin=0, vmax=1, s=10, alpha=1)\n",
    "    ax1.auto_scale_xyz\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_xticklabels([])\n",
    "    # colors = cloud[3:7].numpy().transpose()\n",
    "    # ax1.scatter3D(cloud[0], cloud[1], cloud[2], c=cloud[[6, 3, 4]].numpy().transpose(), s=2, vmin=0, vmax=10)\n",
    "    ax1.set_title(pl_id)\n",
    "    for line in ax1.xaxis.get_ticklines():\n",
    "        line.set_visible(False)\n",
    "    for line in ax1.yaxis.get_ticklines():\n",
    "        line.set_visible(False)\n",
    "\n",
    "\n",
    "    # LV stratum raster\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    color_grad = [(0.8, 0.4, 0.1), (0, 1, 0)]  # first color is white, last is green\n",
    "    cm = colors.LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", color_grad, N=100)\n",
    "    ax2.imshow(image_soil, cmap=cm, vmin=0, vmax=1)\n",
    "    ax2.set_title('Ground level')\n",
    "    ax2.tick_params(\n",
    "        axis='both',  # changes apply to both axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "        bottom=False,  # ticks along the bottom edge are off\n",
    "        top=False,  # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)  # labels along the bottom edge are off\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.set_xticklabels([])\n",
    "\n",
    "\n",
    "    # MV stratum raster\n",
    "    ax3 = fig.add_subplot(gs[1, 2])\n",
    "    color_grad = [(1, 1, 1), (0, 1, 0)]  # first color is white, last is green\n",
    "    cm = colors.LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", color_grad, N=100)\n",
    "    ax3.imshow(image_med_veg, cmap=cm, vmin=0, vmax=1)\n",
    "    ax3.set_title(\"Medium level\")\n",
    "    ax3.tick_params(\n",
    "        axis='both',  # changes apply to the x-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "        bottom=False,  # ticks along the bottom edge are off\n",
    "        top=False,  # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)  # labels along the bottom edge are off\n",
    "    ax3.set_yticklabels([])\n",
    "    ax3.set_xticklabels([])\n",
    "\n",
    "\n",
    "    # Plot high vegetation stratum\n",
    "    ax4 = fig.add_subplot(gs[2, 2])\n",
    "    color_grad = [(1, 1, 1), (0, 1, 0)]  # first color is white, last is red\n",
    "    cm = colors.LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", color_grad, N=100)\n",
    "    ax4.imshow(image_high_veg, cmap=cm, vmin=0, vmax=1)\n",
    "    ax4.set_title(\"High level\")\n",
    "    ax4.tick_params(\n",
    "        axis='both',  # changes apply to the x-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "        bottom=False,  # ticks along the bottom edge are off\n",
    "        top=False,  # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)  # labels along the bottom edge are off\n",
    "    ax4.set_yticklabels([])\n",
    "    ax4.set_xticklabels([])\n",
    "\n",
    "    if txt is not None:\n",
    "        fig.text(.5, .05, txt, ha='center')\n",
    "    plt.savefig(stats_path + pl_id + '_article.svg', format=\"svg\", bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "\n",
    "def visualize(image_soil, image_med_veg, cloud, prediction, pl_id, stats_path, args, txt=None, scores=None, image_high_veg=None):\n",
    "\n",
    "    if image_soil.ndim==3:\n",
    "        image_soil = image_soil[:,:,0]\n",
    "        image_med_veg = image_med_veg[:, :, 0]\n",
    "\n",
    "\n",
    "    # We set figure size depending on the number of subplots\n",
    "    if scores is None and image_high_veg is None:\n",
    "        row, col = 2, 2\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "    else:\n",
    "        row, col = 3, 2\n",
    "        fig = plt.figure(figsize=(20, 25))\n",
    "\n",
    "    # Original point data\n",
    "    ax1 = fig.add_subplot(row, col, 1, projection='3d')\n",
    "    colors_ = cloud[3:6].numpy().transpose()\n",
    "    ax1.scatter(cloud[0], cloud[1], cloud[2]*args.z_max, c=colors_, vmin=0, vmax=1, s=10, alpha=1)\n",
    "    ax1.auto_scale_xyz\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_xticklabels([])\n",
    "    # colors = cloud[3:7].numpy().transpose()\n",
    "    # ax1.scatter3D(cloud[0], cloud[1], cloud[2], c=cloud[[6, 3, 4]].numpy().transpose(), s=2, vmin=0, vmax=10)\n",
    "    ax1.set_title(pl_id)\n",
    "\n",
    "\n",
    "    # LV stratum raster\n",
    "    ax2 = fig.add_subplot(row, col, 2)\n",
    "    color_grad = [(0.8, 0.4, 0.1), (0, 1, 0)]  # first color is brown, last is green\n",
    "    cm = colors.LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", color_grad, N=100)\n",
    "    ax2.imshow(image_soil, cmap=cm, vmin=0, vmax=1)\n",
    "    ax2.set_title('Ground coverage')\n",
    "    ax2.tick_params(\n",
    "        axis='both',  # changes apply to both axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "        bottom=False,  # ticks along the bottom edge are off\n",
    "        top=False,  # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)  # labels along the bottom edge are off\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.set_xticklabels([])\n",
    "\n",
    "\n",
    "    # Pointwise prediction\n",
    "    ax3 = fig.add_subplot(row, col, 3, projection='3d')\n",
    "    ax3.auto_scale_xyz\n",
    "    colors_pred = prediction.cpu().detach().numpy().transpose()\n",
    "    color_matrix = [[0, 1, 0],\n",
    "                    [0.8, 0.4, 0.1],\n",
    "                    [0, 0, 1],\n",
    "                    [1, 0, 0]]\n",
    "    colors_pred = np.matmul(colors_pred, color_matrix)\n",
    "    ax3.scatter(cloud[0], cloud[1], cloud[2]*args.z_max, c=colors_pred, s=10, vmin=0, vmax=1, alpha=1)\n",
    "    ax3.set_title('Pointwise prediction')\n",
    "    ax3.set_yticklabels([])\n",
    "    ax3.set_xticklabels([])\n",
    "\n",
    "\n",
    "    # MV stratum raster\n",
    "    ax4 = fig.add_subplot(row, col, 4)\n",
    "    color_grad = [(1, 1, 1), (0, 0, 1)]  # first color is white, last is blue\n",
    "    cm = colors.LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", color_grad, N=100)\n",
    "    ax4.imshow(image_med_veg, cmap=cm, vmin=0, vmax=1)\n",
    "    ax4.set_title(\"Medium vegetation coverage\")\n",
    "    ax4.tick_params(\n",
    "        axis='both',  # changes apply to the x-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "        bottom=False,  # ticks along the bottom edge are off\n",
    "        top=False,  # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)  # labels along the bottom edge are off\n",
    "    ax4.set_yticklabels([])\n",
    "    ax4.set_xticklabels([])\n",
    "\n",
    "    # Plot stratum scores\n",
    "    if scores is not None:\n",
    "        ax5 = fig.add_subplot(row, col, 5, projection='3d')\n",
    "        ax5.auto_scale_xyz\n",
    "        sc_sum = scores.sum(1)\n",
    "        scores[:, 0] = scores[:, 0] / sc_sum\n",
    "        scores[:, 1] = scores[:, 1] / sc_sum\n",
    "        scores = scores/(scores.max())\n",
    "        colors_pred = scores.transpose(1, 0)[[0, 0, 1], :].cpu().detach().numpy().transpose()\n",
    "        ax5.scatter(cloud[0], cloud[1], cloud[2] * args.z_max, c=colors_pred, s=10, vmin=0, vmax=1)\n",
    "        ax5.set_title(\"Strate scores\")\n",
    "        ax5.set_yticklabels([])\n",
    "        ax5.set_xticklabels([])\n",
    "\n",
    "\n",
    "    # Plot high vegetation stratum\n",
    "    if image_high_veg is not None:\n",
    "        ax6 = fig.add_subplot(row, col, 6)\n",
    "        color_grad = [(1, 1, 1), (1, 0, 0)]  # first color is white, last is red\n",
    "        cm = colors.LinearSegmentedColormap.from_list(\n",
    "            \"Custom\", color_grad, N=100)\n",
    "        ax6.imshow(image_high_veg, cmap=cm, vmin=0, vmax=1)\n",
    "        ax6.set_title(\"High vegetation coverage\")\n",
    "        ax6.tick_params(\n",
    "            axis='both',  # changes apply to the x-axis\n",
    "            which='both',  # both major and minor ticks are affected\n",
    "            bottom=False,  # ticks along the bottom edge are off\n",
    "            top=False,  # ticks along the top edge are off\n",
    "            left=False,\n",
    "            right=False,\n",
    "            labelbottom=False)  # labels along the bottom edge are off\n",
    "        ax6.set_yticklabels([])\n",
    "        ax6.set_xticklabels([])\n",
    "\n",
    "    if txt is not None:\n",
    "        fig.text(.5, .05, txt, ha='center')\n",
    "    plt.savefig(stats_path + pl_id + '.png', format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def create_final_images(pred_pl, gt, pred_pointwise_b, cloud, likelihood, plot_name, mean_dataset, stats_path,\n",
    "                        stats_file, args, create_raster=True, adm=None):\n",
    "    '''\n",
    "    We do final data reprojection to the 2D space (2 stratum - ground vegetation level and medium level, optionally high level) by associating the points to the pixels.\n",
    "    Then we create the images with those stratum\n",
    "    '''\n",
    "    for b in range(len(pred_pointwise_b)):\n",
    "        # we get prediction stats string\n",
    "        pred_cloud = pred_pointwise_b[b]\n",
    "        current_cloud = cloud[b]\n",
    "        # we do raster reprojection, but we do not use torch scatter as we have to associate each value to a pixel\n",
    "        xy = current_cloud[:2]\n",
    "        xy = torch.floor((xy - torch.min(xy, dim=1).values.view(2, 1).expand_as(xy)) / (\n",
    "                torch.max(xy, dim=1).values - torch.min(xy, dim=1).values + 0.0001).view(2, 1).expand_as(\n",
    "            xy) * args.diam_pix).int()\n",
    "        xy = xy.cpu().numpy()\n",
    "        unique, index, inverse = np.unique(xy.T, axis=0, return_index=True, return_inverse=True)\n",
    "\n",
    "        # we get the values for each unique pixel and write them to rasters\n",
    "        image_ground = np.full((args.diam_pix, args.diam_pix), np.nan)\n",
    "        image_med_veg = np.full((args.diam_pix, args.diam_pix), np.nan)\n",
    "        if args.nb_stratum == 3:\n",
    "            image_high_veg = np.full((args.diam_pix, args.diam_pix), np.nan)\n",
    "        else:\n",
    "            image_high_veg = None\n",
    "        for i in np.unique(inverse):\n",
    "            where = np.where(inverse == i)[0]\n",
    "            k, m = xy.T[where][0]\n",
    "            maxpool = nn.MaxPool1d(len(where))\n",
    "            max_pool_val = maxpool(pred_cloud[:, where].unsqueeze(0)).cpu().detach().numpy().flatten()\n",
    "\n",
    "            if args.norm_ground:  # we normalize ground level coverage values\n",
    "                proba_low_veg = max_pool_val[0] / (max_pool_val[:2].sum())\n",
    "            else:   # we do not normalize anything, as bare soil coverage does not participate in absolute loss\n",
    "                proba_low_veg = max_pool_val[0]\n",
    "            proba_med_veg = max_pool_val[2]\n",
    "            image_ground[m, k] = proba_low_veg\n",
    "            image_med_veg[m, k] = proba_med_veg\n",
    "\n",
    "            if args.nb_stratum == 3:\n",
    "                proba_high_veg = max_pool_val[3]\n",
    "                image_high_veg[m, k] = proba_high_veg\n",
    "        image_ground = np.flip(image_ground, axis=0)  # we flip along y axis as the 1st raster row starts with 0\n",
    "        image_med_veg = np.flip(image_med_veg, axis=0)\n",
    "        if args.nb_stratum == 3:\n",
    "            image_high_veg = np.flip(image_high_veg, axis=0)\n",
    "        # We normalize back x,y values to create a tiff file with 2 rasters\n",
    "        if create_raster:\n",
    "            xy = xy * 10 + np.asarray(mean_dataset[plot_name]).reshape(-1, 1)\n",
    "            geo = [np.min(xy, axis=1)[0], (np.max(xy, axis=1)[0] - np.min(xy, axis=1)[0]) / args.diam_pix, 0,\n",
    "                   np.max(xy, axis=1)[1], 0, (-np.max(xy, axis=1)[1] + np.min(xy, axis=1)[1]) / args.diam_pix]\n",
    "            if args.nb_stratum == 2:\n",
    "                img_to_write = np.concatenate(([image_ground], [image_med_veg]), 0)\n",
    "            else:\n",
    "                img_to_write = np.concatenate(([image_ground], [image_med_veg], [image_high_veg]), 0)\n",
    "            create_tiff(nb_channels=args.nb_stratum, new_tiff_name=stats_path + plot_name + \".tif\", width=args.diam_pix,\n",
    "                        height=args.diam_pix, datatype=gdal.GDT_Float32, data_array=img_to_write, geotransformation=geo)\n",
    "        if args.adm:\n",
    "            text = 'Pred ' + np.array2string(\n",
    "                np.round(np.asarray(pred_pl[b].cpu().detach().numpy().reshape(-1)), 2)) + ' ADM ' + str(\n",
    "                adm[b].cpu().detach().numpy().round(2)) + ' GT ' + np.array2string(\n",
    "                gt.cpu().numpy()[0])  # prediction text\n",
    "        else:\n",
    "            text = ' Pred ' + np.array2string(\n",
    "                np.round(np.asarray(pred_pl[b].cpu().detach().numpy().reshape(-1)), 2)) + ' GT ' + np.array2string(\n",
    "                gt.cpu().numpy()[0])\n",
    "        print_stats(stats_file, plot_name + \" \" + text, print_to_console=True)\n",
    "        # We create an image with 5 or 6 subplots:\n",
    "        # 1. original point cloud, 2. LV image, 3. pointwise prediction point cloud, 4. MV image, 5.Stratum probabilities point cloud, 6.(optional) HV image\n",
    "        visualize(image_ground, image_med_veg, current_cloud, pred_cloud, plot_name, stats_path, args, txt=text,\n",
    "                  scores=likelihood, image_high_veg=image_high_veg)\n",
    "        if args.nb_stratum==3:\n",
    "            visualize_article(image_ground, image_med_veg, image_high_veg, current_cloud, plot_name, stats_path, args, txt=text)\n",
    "\n",
    "\n",
    "# We create a tiff file with 2 or 3 stratum\n",
    "def create_tiff(nb_channels, new_tiff_name, width, height, datatype, data_array, geotransformation):\n",
    "    # We set Lambert 93 projection\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(2154)\n",
    "    proj = srs.ExportToWkt()\n",
    "    # We create a datasource\n",
    "    driver_tiff = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver_tiff.Create(new_tiff_name, width, height, nb_channels, datatype)\n",
    "    if nb_channels == 1:\n",
    "        dst_ds.GetRasterBand(1).WriteArray(data_array)\n",
    "    else:\n",
    "        for ch in range(nb_channels):\n",
    "            dst_ds.GetRasterBand(ch + 1).WriteArray(data_array[ch])\n",
    "    dst_ds.SetGeoTransform(geotransformation)\n",
    "    dst_ds.SetProjection(proj)\n",
    "    return dst_ds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PointNet model for point classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The PointNet network for semantic segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, MLP_1, MLP_2, MLP_3, args):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        MLP_1, LMP2 and MLP3 = int array, size of the layers of multi-layer perceptrons\n",
    "        for example MLP1 = [32,64]\n",
    "        n_class = int,  the number of class\n",
    "        input_feat = int, number of input feature\n",
    "        subsample_size = int, number of points to which the tiles are subsampled\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(PointNet, self).__init__()  # necessary for all classes extending the module class\n",
    "\n",
    "        self.is_cuda = args.cuda\n",
    "        self.subsample_size = args.subsample_size\n",
    "        self.n_class = args.n_class\n",
    "        self.drop = args.drop\n",
    "        self.soft = args.soft\n",
    "        self.input_feat = args.n_input_feats\n",
    "\n",
    "        # since we don't know the number of layers in the MLPs, we need to use loops\n",
    "        # to create the correct number of layers\n",
    "        m1 = MLP_1[-1]  # size of the first embeding F1\n",
    "        m2 = MLP_2[-1]  # size of the second embeding F2\n",
    "\n",
    "        # build MLP_1: input [input_feat x n] -> f1 [m1 x n]\n",
    "        modules = []\n",
    "        for i in range(len(MLP_1)):  # loop over the layer of MLP1\n",
    "            # note: for the first layer, the first in_channels is feature_size\n",
    "            modules.append(\n",
    "                nn.Conv1d(in_channels=MLP_1[i - 1] if i > 0 else self.input_feat, out_channels=MLP_1[i], kernel_size=1))\n",
    "            modules.append(nn.BatchNorm1d(MLP_1[i]))\n",
    "            modules.append(nn.ReLU(True))\n",
    "        # this transform the list of layers into a callable module\n",
    "        self.MLP_1 = nn.Sequential(*modules)\n",
    "\n",
    "        # build MLP_2: f1 [m1 x n] -> f2 [m2 x n]\n",
    "        modules = []\n",
    "        for i in range(len(MLP_2)):\n",
    "            modules.append(nn.Conv1d(in_channels=MLP_2[i - 1] if i > 0 else m1, out_channels=MLP_2[i], kernel_size=1))\n",
    "            modules.append(nn.BatchNorm1d(MLP_2[i]))\n",
    "            modules.append(nn.ReLU(True))\n",
    "        self.MLP_2 = nn.Sequential(*modules)\n",
    "\n",
    "        # build MLP_3: f1 [(m1 + m2) x n] -> output [k x n]\n",
    "        modules = []\n",
    "        for i in range(len(MLP_3)):\n",
    "            modules.append(\n",
    "                nn.Conv1d(in_channels=MLP_3[i - 1] if i > 0 else (m1 + m2), out_channels=MLP_3[i], kernel_size=1))\n",
    "            modules.append(nn.BatchNorm1d(MLP_3[i]))\n",
    "            modules.append(nn.ReLU(True))\n",
    "        # note: the last layer do not have normalization nor activation\n",
    "        modules.append(nn.Dropout(p=self.drop))\n",
    "        modules.append(nn.Conv1d(MLP_3[-1], self.n_class, 1))\n",
    "        self.MLP_3 = nn.Sequential(*modules)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(self.subsample_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self = self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        the forward function producing the embeddings for each point of 'input'\n",
    "        input = [n_batch, input_feat, subsample_size] float array: input features\n",
    "        output = [n_batch,n_class, subsample_size] float array: point class logits\n",
    "        \"\"\"\n",
    "        # print(input.size())\n",
    "        if self.is_cuda:\n",
    "            input = input.cuda()\n",
    "        f1 = self.MLP_1(input)\n",
    "        f2 = self.MLP_2(f1)\n",
    "        G = self.maxpool(f2)\n",
    "        Gf1 = torch.cat((G.repeat(1, 1, self.subsample_size), f1), 1)\n",
    "        out_pointwise = self.MLP_3(Gf1)\n",
    "        if self.soft:\n",
    "            out_pointwise = self.softmax(out_pointwise)\n",
    "        else:\n",
    "            out_pointwise = self.sigmoid(out_pointwise)\n",
    "        return out_pointwise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Different functions to keep track of statistics:\n",
    "    - Per epoch\n",
    "    - Per Fold\n",
    "    - Mean for the whole dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# We compute all possible mean stats per loss for all folds\n",
    "def stats_for_all_folds(all_folds_loss_train_lists, all_folds_loss_test_lists, stats_file, args):\n",
    "    loss_train_list, loss_train_abs_list, loss_train_log_list, loss_train_adm_list = all_folds_loss_train_lists\n",
    "    loss_test_list, loss_test_abs_list, loss_test_log_list, loss_test_abs_gl_list, loss_test_abs_ml_list, loss_test_abs_hl_list, loss_test_adm_list = all_folds_loss_test_lists\n",
    "\n",
    "\n",
    "    if args.adm:\n",
    "        mean_cross_fold_train = np.mean(loss_train_list), np.mean(loss_train_abs_list), np.mean(\n",
    "            loss_train_log_list), np.mean(loss_train_adm_list)\n",
    "        print_stats(stats_file,\n",
    "                    \"Mean Train Loss \" + str(mean_cross_fold_train[0]) + \" Loss abs \" + str(\n",
    "                        mean_cross_fold_train[1]) + \" Loss log \" + str(mean_cross_fold_train[2]) + \" Loss ADM \" + str(\n",
    "                        mean_cross_fold_train[3]),\n",
    "                    print_to_console=True)\n",
    "\n",
    "        if args.nb_stratum == 2:\n",
    "            mean_cross_fold_test = np.mean(loss_test_list), np.mean(loss_test_abs_list), np.mean(\n",
    "                loss_test_log_list), np.mean(loss_test_abs_gl_list), np.mean(loss_test_abs_ml_list), np.mean(\n",
    "                loss_test_adm_list)\n",
    "\n",
    "            print_stats(stats_file,\n",
    "                        \"Mean Test Loss \" + str(mean_cross_fold_test[0]) + \" Loss abs \" + str(\n",
    "                            mean_cross_fold_test[1]) + \" Loss log \" + str(\n",
    "                            mean_cross_fold_test[2]) + \" Loss abs GL \" + str(\n",
    "                            mean_cross_fold_test[3]) + \" Loss abs ML \" + str(\n",
    "                            mean_cross_fold_test[4]) + \" Loss ADM \" + str(mean_cross_fold_test[5]),\n",
    "                        print_to_console=True)\n",
    "\n",
    "        else:  # 3 stratum\n",
    "            mean_cross_fold_test = np.mean(loss_test_list), np.mean(loss_test_abs_list), np.mean(\n",
    "                loss_test_log_list), np.mean(loss_test_abs_gl_list), np.mean(loss_test_abs_ml_list), np.mean(\n",
    "                loss_test_abs_hl_list), np.mean(loss_test_adm_list)\n",
    "\n",
    "            print_stats(stats_file,\n",
    "                        \"Mean Test Loss \" + str(mean_cross_fold_test[0]) + \" Loss abs \" + str(\n",
    "                            mean_cross_fold_test[1]) + \" Loss log \" + str(\n",
    "                            mean_cross_fold_test[2]) + \" Loss abs GL \" + str(\n",
    "                            mean_cross_fold_test[3]) + \" Loss abs ML \" + str(\n",
    "                            mean_cross_fold_test[4]) + \" Loss abs HL \" + str(\n",
    "                            mean_cross_fold_test[5]) + \" Loss ADM \" + str(mean_cross_fold_test[6]),\n",
    "                        print_to_console=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        mean_cross_fold_train = np.mean(loss_train_list), np.mean(loss_train_abs_list), np.mean(loss_train_log_list)\n",
    "        print_stats(stats_file,\n",
    "                    \"Mean Train Loss \" + str(mean_cross_fold_train[0]) + \" Loss abs \" + str(\n",
    "                        mean_cross_fold_train[1]) + \" Loss log \" + str(mean_cross_fold_train[2]),\n",
    "                    print_to_console=True)\n",
    "\n",
    "        if args.nb_stratum == 2:\n",
    "            mean_cross_fold_test = np.mean(loss_test_list), np.mean(loss_test_abs_list), np.mean(\n",
    "                loss_test_log_list), np.mean(loss_test_abs_gl_list), np.mean(loss_test_abs_ml_list)\n",
    "\n",
    "            print_stats(stats_file,\n",
    "                        \"Mean Test Loss \" + str(mean_cross_fold_test[0]) + \" Loss abs \" + str(\n",
    "                            mean_cross_fold_test[1]) + \" Loss log \" + str(\n",
    "                            mean_cross_fold_test[2]) + \" Loss abs GL \" + str(\n",
    "                            mean_cross_fold_test[3]) + \" Loss abs ML \" + str(\n",
    "                            mean_cross_fold_test[4]),\n",
    "                        print_to_console=True)\n",
    "\n",
    "        else:  # 3 stratum\n",
    "            mean_cross_fold_test = np.mean(loss_test_list), np.mean(loss_test_abs_list), np.mean(\n",
    "                loss_test_log_list), np.mean(loss_test_abs_gl_list), np.mean(loss_test_abs_ml_list), np.mean(\n",
    "                loss_test_abs_hl_list)\n",
    "\n",
    "            print_stats(stats_file,\n",
    "                        \"Mean Test Loss \" + str(mean_cross_fold_test[0]) + \" Loss abs \" + str(\n",
    "                            mean_cross_fold_test[1]) + \" Loss log \" + str(\n",
    "                            mean_cross_fold_test[2]) + \" Loss abs GL \" + str(\n",
    "                            mean_cross_fold_test[3]) + \" Loss abs ML \" + str(\n",
    "                            mean_cross_fold_test[4]) + \" Loss abs HL \" + str(\n",
    "                            mean_cross_fold_test[5]),\n",
    "                        print_to_console=True)\n",
    "\n",
    "\n",
    "\n",
    "# We compute all possible loss stats per fold\n",
    "def stats_per_fold(all_folds_loss_train_lists, all_folds_loss_test_lists, final_train_losses_list, final_test_losses_list, stats_file, fold_id, args):\n",
    "    if all_folds_loss_test_lists is None and all_folds_loss_test_lists is None:\n",
    "        # We keep track of stats per fold\n",
    "        loss_train_list = []\n",
    "        loss_train_abs_list = []\n",
    "        loss_train_log_list = []\n",
    "        loss_train_adm_list = []\n",
    "        loss_test_list = []\n",
    "        loss_test_abs_list = []\n",
    "        loss_test_log_list = []\n",
    "        loss_test_abs_gl_list = []\n",
    "        loss_test_abs_ml_list = []\n",
    "        loss_test_abs_hl_list = []\n",
    "        loss_test_adm_list = []\n",
    "    else:\n",
    "        loss_train_list, loss_train_abs_list, loss_train_log_list, loss_train_adm_list = all_folds_loss_train_lists\n",
    "        loss_test_list, loss_test_abs_list, loss_test_log_list, loss_test_abs_gl_list, loss_test_abs_ml_list, loss_test_abs_hl_list, loss_test_adm_list = all_folds_loss_test_lists\n",
    "\n",
    "\n",
    "    loss_train, loss_train_abs, loss_train_log, loss_train_adm = final_train_losses_list\n",
    "    loss_test, loss_test_abs, loss_test_log, loss_test_abs_gl, loss_test_abs_ml, loss_test_abs_hl, loss_test_adm = final_test_losses_list\n",
    "\n",
    "    # Save all loss stats\n",
    "    print_stats(stats_file,\n",
    "                \"Fold_\" + str(fold_id) + \" Train Loss \" + str(loss_train) + \" Loss abs \" + str(\n",
    "                    loss_train_abs) + \" Loss log \" + str(loss_train_log),\n",
    "                print_to_console=True)\n",
    "    if args.adm:\n",
    "        print_stats(stats_file,\n",
    "                    \"Fold_\" + str(fold_id) + \" Test Loss \" + str(loss_test) + \" Loss abs \" + str(\n",
    "                        loss_test_abs) + \" Loss log \" + str(loss_test_log) + \" Loss abs adm \" + str(loss_test_adm),\n",
    "                    print_to_console=True)\n",
    "    else:\n",
    "        print_stats(stats_file,\n",
    "                    \"Fold_\" + str(fold_id) + \" Test Loss \" + str(loss_test) + \" Loss abs \" + str(\n",
    "                        loss_test_abs) + \" Loss log \" + str(loss_test_log),\n",
    "                    print_to_console=True)\n",
    "\n",
    "    if args.nb_stratum == 2:\n",
    "        print_stats(stats_file,\n",
    "                    \"Fold_\" + str(fold_id) + \" Test Loss abs GL \" + str(loss_test_abs_gl) + \" Test Loss abs ML \" + str(\n",
    "                        loss_test_abs_ml), print_to_console=True)\n",
    "    else:\n",
    "        print_stats(stats_file,\n",
    "                    \"Fold_\" + str(fold_id) + \" Test Loss abs GL \" + str(loss_test_abs_gl) + \" Test Loss abs ML \" + str(\n",
    "                        loss_test_abs_ml) + \" Test Loss abs HL \" + str(\n",
    "                        loss_test_abs_hl), print_to_console=True)\n",
    "\n",
    "    loss_train_list.append(loss_train)\n",
    "    loss_train_abs_list.append(loss_train_abs)\n",
    "    loss_train_log_list.append(loss_train_log)\n",
    "    loss_train_adm_list.append(loss_train_adm)\n",
    "\n",
    "    loss_test_list.append(loss_test)\n",
    "    loss_test_abs_list.append(loss_test_abs)\n",
    "    loss_test_log_list.append(loss_test_log)\n",
    "    loss_test_abs_gl_list.append(loss_test_abs_gl)\n",
    "    loss_test_abs_ml_list.append(loss_test_abs_ml)\n",
    "    loss_test_abs_hl_list.append(loss_test_abs_hl)\n",
    "    loss_test_adm_list.append(loss_test_adm)\n",
    "\n",
    "    all_folds_loss_train_lists = [loss_train_list, loss_train_abs_list, loss_train_log_list, loss_train_adm_list]\n",
    "    all_folds_loss_test_lists = [loss_test_list, loss_test_abs_list, loss_test_log_list, loss_test_abs_gl_list, loss_test_abs_ml_list, loss_test_abs_hl_list, loss_test_adm_list]\n",
    "    return all_folds_loss_train_lists, all_folds_loss_test_lists\n",
    "\n",
    "\n",
    "\n",
    "#We perform tensorboard visualisation by writing the stats of each epoch to the writer\n",
    "def write_to_writer(writer, args, i_epoch, list_with_losses, train):\n",
    "    TESTCOLOR = '\\033[104m'\n",
    "    TRAINCOLOR = '\\033[100m'\n",
    "    NORMALCOLOR = '\\033[0m'\n",
    "\n",
    "    if train:\n",
    "        loss_train, loss_train_abs, loss_train_log, loss_train_adm = list_with_losses\n",
    "        if args.adm:\n",
    "            print(\n",
    "                TRAINCOLOR + 'Epoch %3d -> Train Loss: %1.4f Train Loss Abs: %1.4f Train Loss Log: %1.4f Train Loss Adm: %1.4f' % (\n",
    "                i_epoch, loss_train, loss_train_abs, loss_train_log, loss_train_adm) + NORMALCOLOR)\n",
    "            writer.add_scalar('Loss/train_abs_adm', loss_train_adm, i_epoch + 1)\n",
    "        else:\n",
    "            print(TRAINCOLOR + 'Epoch %3d -> Train Loss: %1.4f Train Loss Abs: %1.4f Train Loss Log: %1.4f' % (\n",
    "            i_epoch, loss_train, loss_train_abs, loss_train_log) + NORMALCOLOR)\n",
    "        writer.add_scalar('Loss/train', loss_train, i_epoch + 1)\n",
    "        writer.add_scalar('Loss/train_abs', loss_train_abs, i_epoch + 1)\n",
    "        writer.add_scalar('Loss/train_log', loss_train_log, i_epoch + 1)\n",
    "\n",
    "    else:\n",
    "        loss_test, loss_test_abs, loss_test_log, _, _, _, loss_test_adm = list_with_losses\n",
    "        if args.adm:\n",
    "            print(TESTCOLOR + 'Test Loss: %1.4f Test Loss Abs: %1.4f Test Loss Log: %1.4f Test Loss Adm: %1.4f' % (\n",
    "            loss_test, loss_test_abs, loss_test_log, loss_test_adm) + NORMALCOLOR)\n",
    "            writer.add_scalar('Loss/test_abs_adm', loss_test_adm, i_epoch + 1)\n",
    "        else:\n",
    "            print(TESTCOLOR + 'Test Loss: %1.4f Test Loss Abs: %1.4f Test Loss Log: %1.4f' % (\n",
    "                loss_test, loss_test_abs, loss_test_log) + NORMALCOLOR)\n",
    "        writer.add_scalar('Loss/test', loss_test, i_epoch + 1)\n",
    "        writer.add_scalar('Loss/test_abs', loss_test_abs, i_epoch + 1)\n",
    "        writer.add_scalar('Loss/test_log', loss_test_log, i_epoch + 1)\n",
    "    return writer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we perform 3D data projection into different stratum and compute the vegetation ratio of each stratum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def project_to_2d(pred_pointwise, cloud, pred_pointwise_b, PCC, args):\n",
    "    \"\"\"\n",
    "    We do all the computation to obtain\n",
    "    pred_pl - [Bx4] prediction vector for the plot\n",
    "    scores -  [(BxN)x2] probas_ground_nonground that a point belongs to stratum 1 or stratum 2\n",
    "    \"\"\"\n",
    "    index_batches = []\n",
    "    index_group = []\n",
    "    batches_len = []\n",
    "\n",
    "    # we project 3D points to 2D plane\n",
    "    # We use torch scatter to process\n",
    "    for b in range(len(pred_pointwise_b)):\n",
    "        current_cloud = cloud[b]\n",
    "        xy = current_cloud[:2]\n",
    "        xy = torch.floor((xy - torch.min(xy, dim=1).values.view(2, 1).expand_as(xy)) / (\n",
    "                torch.max(xy, dim=1).values - torch.min(xy, dim=1).values + 0.0001).view(2, 1).expand_as(\n",
    "            xy) * args.diam_pix).int()\n",
    "\n",
    "        unique, index = torch.unique(xy.T, dim=0, return_inverse=True)\n",
    "        index_b = torch.full(torch.unique(index).size(), b)\n",
    "        if PCC.is_cuda:\n",
    "            index = index.cuda()\n",
    "            index_b = index_b.cuda()\n",
    "        index = index + np.asarray(batches_len).sum()\n",
    "        index_batches.append(index.type(torch.LongTensor))\n",
    "        index_group.append(index_b.type(torch.LongTensor))\n",
    "        batches_len.append(torch.unique(index).size(0))\n",
    "    index_batches = torch.cat(index_batches)\n",
    "    index_group = torch.cat(index_group)\n",
    "    if PCC.is_cuda:\n",
    "        index_batches = index_batches.cuda()\n",
    "        index_group = index_group.cuda()\n",
    "    pixel_max = scatter_max(pred_pointwise.T, index_batches)[0]\n",
    "\n",
    "    # We compute prediction values per pixel\n",
    "    if args.norm_ground:    # we normalize ground level coverage values, so c_low[i]+c_bare[i]=1\n",
    "        c_low_veg_pix = pixel_max[0, :] / (pixel_max[:2, :].sum(0))\n",
    "        c_bare_soil_pix = pixel_max[1, :] / (pixel_max[:2, :].sum(0))\n",
    "    else:   # we do not normalize anything, as bare soil coverage does not participate in absolute loss\n",
    "        c_low_veg_pix = pixel_max[0, :]\n",
    "        c_bare_soil_pix = 1 - c_low_veg_pix\n",
    "    c_med_veg_pix = pixel_max[2, :]\n",
    "\n",
    "    if args.nb_stratum==2:\n",
    "        # We compute prediction values per plot\n",
    "        c_low_veg = scatter_mean(c_low_veg_pix, index_group)\n",
    "        c_bare_soil = scatter_mean(c_bare_soil_pix, index_group)\n",
    "        c_med_veg = scatter_mean(c_med_veg_pix, index_group)\n",
    "        # c_other = scatter_mean(c_other_pix, index_group)\n",
    "        pred_pl = torch.stack([c_low_veg, c_bare_soil, c_med_veg]).T\n",
    "\n",
    "    else:   # 3 stratum\n",
    "        c_high_veg_pix = pixel_max[3, :]    # we equally compute raster for high vegetation\n",
    "\n",
    "        # We compute prediction values per plot\n",
    "        c_low_veg = scatter_mean(c_low_veg_pix, index_group)\n",
    "        c_bare_soil = scatter_mean(c_bare_soil_pix, index_group)\n",
    "        c_med_veg = scatter_mean(c_med_veg_pix, index_group)\n",
    "        c_high_veg = scatter_mean(c_high_veg_pix, index_group)\n",
    "        pred_pl = torch.stack([c_low_veg, c_bare_soil, c_med_veg, c_high_veg]).T\n",
    "\n",
    "    if args.adm:\n",
    "        c_adm_pix = torch.max(pixel_max[[0,2], :], dim=0)[0]\n",
    "        c_adm = scatter_mean(c_adm_pix, index_group)\n",
    "    else:\n",
    "        c_adm = None\n",
    "\n",
    "    return pred_pl, c_adm\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One epoch model training function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, PCC, train_set, params, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(train_set, collate_fn=cloud_collate, \\\n",
    "                                         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_log = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs_adm = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index_batch, (cloud, gt) in enumerate(loader):\n",
    "\n",
    "        if PCC.is_cuda:\n",
    "            gt = gt.cuda()\n",
    "\n",
    "        optimizer.zero_grad()  # put gradient to zero\n",
    "        pred_pointwise, pred_pointwise_b = PCC.run(model, cloud)  # compute the pointwise prediction\n",
    "        pred_pl, pred_adm = project_to_2d(pred_pointwise, cloud, pred_pointwise_b, PCC, args)  # compute plot prediction\n",
    "\n",
    "        # we compute two losses (negative loglikelihood and the absolute error loss for 2 or 3 stratum)\n",
    "        loss_abs = loss_absolute(pred_pl, gt, args)\n",
    "        loss_log, likelihood = loss_loglikelihood(pred_pointwise, cloud, params, PCC,\n",
    "                                                  args)  # negative loglikelihood loss\n",
    "        if args.adm:\n",
    "            # we compute admissibility loss\n",
    "            loss_adm = loss_abs_adm(pred_adm, gt)\n",
    "            loss = loss_abs + args.m * loss_log + 0.5 * loss_adm\n",
    "            loss_meter_abs_adm.add(loss_adm.item())\n",
    "        else:\n",
    "            loss = loss_abs + args.m * loss_log\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_meter_abs.add(loss_abs.item())\n",
    "        loss_meter_log.add(loss_log.item())\n",
    "        loss_meter.add(loss.item())\n",
    "        gc.collect()\n",
    "\n",
    "    return loss_meter.value()[0], loss_meter_abs.value()[0], loss_meter_log.value()[0], loss_meter_abs_adm.value()[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "One epoch model evaluation function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "\n",
    "# We import from other files\n",
    "from utils.create_final_images import *\n",
    "from data_loader.loader import *\n",
    "from utils.reproject_to_2d_and_predict_plot_coverage import *\n",
    "from model.loss_functions import *\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def eval(model, PCC, test_set, params, args, test_list, mean_dataset, stats_path, stats_file, last_epoch=False):\n",
    "    \"\"\"eval on test set\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(test_set, collate_fn=cloud_collate, batch_size=1, shuffle=False)\n",
    "    loss_meter_abs = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_log = tnt.meter.AverageValueMeter()\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs_gl = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs_ml = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs_hl = tnt.meter.AverageValueMeter()\n",
    "    loss_meter_abs_adm = tnt.meter.AverageValueMeter()\n",
    "\n",
    "\n",
    "    for index_batch, (cloud, gt) in enumerate(loader):\n",
    "\n",
    "        if PCC.is_cuda:\n",
    "            gt = gt.cuda()\n",
    "\n",
    "\n",
    "        start_encoding_time = time.time()\n",
    "        pred_pointwise, pred_pointwise_b = PCC.run(model, cloud)  # compute the prediction\n",
    "        end_encoding_time = time.time()\n",
    "        if last_epoch:            # if it is the last epoch, we get time stats info\n",
    "            print(end_encoding_time - start_encoding_time)\n",
    "\n",
    "\n",
    "        pred_pl, pred_adm = project_to_2d(pred_pointwise, cloud, pred_pointwise_b, PCC, args) # compute plot prediction\n",
    "\n",
    "        # we compute two losses (negative loglikelihood and the absolute error loss for 2 stratum)\n",
    "        loss_abs = loss_absolute(pred_pl, gt, args)  # absolut loss\n",
    "        loss_log, likelihood = loss_loglikelihood(pred_pointwise, cloud, params, PCC,\n",
    "                                                  args)  # negative loglikelihood loss\n",
    "        if args.adm:\n",
    "            # we compute admissibility loss\n",
    "            loss_adm = loss_abs_adm(pred_adm, gt)\n",
    "            loss = loss_abs + args.m * loss_log + 0.5 * loss_adm\n",
    "            loss_meter_abs_adm.add(loss_adm.item())\n",
    "        else:\n",
    "            loss = loss_abs + args.m * loss_log\n",
    "\n",
    "        loss_meter.add(loss.item())\n",
    "        loss_meter_abs.add(loss_abs.item())\n",
    "        loss_meter_log.add(loss_log.item())\n",
    "        gc.collect()\n",
    "\n",
    "        if last_epoch:\n",
    "\n",
    "            component_losses = loss_absolute(pred_pl, gt, args, level_loss=True) # gl_mv_loss gives separated losses for each stratum\n",
    "            if args.nb_stratum == 2:\n",
    "                loss_abs_gl, loss_abs_ml = component_losses\n",
    "            else:\n",
    "                loss_abs_gl, loss_abs_ml, loss_abs_hl = component_losses\n",
    "                loss_abs_hl = loss_abs_hl[~torch.isnan(loss_abs_hl)]\n",
    "                if loss_abs_hl.size(0) > 0:\n",
    "                    loss_meter_abs_hl.add(loss_abs_hl.item())\n",
    "            loss_meter_abs_gl.add(loss_abs_gl.item())\n",
    "            loss_meter_abs_ml.add(loss_abs_ml.item())\n",
    "\n",
    "            create_final_images(pred_pl, gt, pred_pointwise_b, cloud, likelihood, test_list[index_batch], mean_dataset, stats_path, stats_file,\n",
    "                                args, adm=pred_adm)  # create final images with stratum values\n",
    "\n",
    "\n",
    "    return loss_meter.value()[0], loss_meter_abs.value()[0], loss_meter_log.value()[0], loss_meter_abs_gl.value()[0], loss_meter_abs_ml.value()[0], loss_meter_abs_hl.value()[0], loss_meter_abs_adm.value()[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For efficiency concerns, we want all the point clouds we consider to have the same number of points `subsample_size`. This allows for easy GPU parallelization. As the tiles are of different sizes, we sample a fixed number of points from each tile (with replacement). The labels of the sampled points are then spatially interpolated to the original point cloud using a nearest neighbor strategy.\n",
    "`PointCloudClassifier` class takes care of sampling, batching, prediciton, and interpolation of differently-sized clouds.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class PointCloudClassifier:\n",
    "    \"\"\"\n",
    "    The main point cloud classifier Class\n",
    "    deal with subsampling the tiles to a fixed number of points\n",
    "    and interpolating to the original clouds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.subsample_size = args.subsample_size  # number of points to subsample each point cloud in the batches\n",
    "        self.n_input_feats = 3  # size of the point descriptors in input\n",
    "        if len(args.input_feats) > 3:\n",
    "            self.n_input_feats = len(args.input_feats)\n",
    "        self.n_class = args.n_class  # number of classes in the output\n",
    "        self.is_cuda = args.cuda  # wether to use GPU acceleration\n",
    "\n",
    "    def run(self, model, clouds):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        model = the neural network\n",
    "        clouds = list of n_batch tensors of size [n_feat, n_points_i]: batch of point clouds\n",
    "        OUTPUT:\n",
    "        pred = [sum_i n_points_i, n_class] float tensor : prediction for each element of the\n",
    "             batch in a single tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # number of clouds in the batch #TYPO\n",
    "        n_batch = len(clouds)\n",
    "        # will contain the prediction for all clouds in the batch\n",
    "        prediction_batch = torch.zeros((self.n_class, 0))\n",
    "\n",
    "        # batch_data contain all the clouds in the batch subsampled to self.subsample_size points\n",
    "        sampled_clouds = torch.Tensor(n_batch, self.n_input_feats, self.subsample_size)\n",
    "        if self.is_cuda:\n",
    "            sampled_clouds = sampled_clouds.cuda()\n",
    "            prediction_batch = prediction_batch.cuda()\n",
    "\n",
    "        # build batches of the same size\n",
    "        for i_batch in range(n_batch):\n",
    "            # load the elements in the batch one by one and subsample/ oversample them\n",
    "            # to a size of self.subsample_size points\n",
    "\n",
    "            cloud = clouds[i_batch][:int(self.n_input_feats), :]\n",
    "            n_points = cloud.shape[1]  # number of points in the considered cloud\n",
    "            if n_points > self.subsample_size:\n",
    "                selected_points = np.random.choice(n_points, self.subsample_size,\n",
    "                                                   replace=False)\n",
    "            else:\n",
    "                selected_points = np.random.choice(n_points, self.subsample_size,\n",
    "                                                   replace=True)\n",
    "            cloud = cloud[:, selected_points]  # reduce the current cloud to the selected points\n",
    "\n",
    "            sampled_clouds[i_batch, :, :] = cloud.clone()  # place current sampled cloud in sampled_clouds\n",
    "\n",
    "        point_prediction = model(sampled_clouds)  # classify the batch of sampled clouds\n",
    "        assert (point_prediction.shape == torch.Size([n_batch, self.n_class, self.subsample_size]))\n",
    "\n",
    "        # interpolation to original point clouds\n",
    "        prediction_batches = []\n",
    "        for i_batch in range(n_batch):\n",
    "            # get the original point clouds positions\n",
    "            cloud = clouds[i_batch]\n",
    "            # and the corresponding sampled batch (only xyz position)\n",
    "            sampled_cloud = sampled_clouds[i_batch, :3, :]\n",
    "            n_points = cloud.shape[1]\n",
    "            knn = NearestNeighbors(1, algorithm='kd_tree').fit( \\\n",
    "                sampled_cloud.cpu().permute(1, 0))\n",
    "            # select for each point in the original point cloud the closest point in sampled_cloud\n",
    "            _, closest_point = knn.kneighbors(cloud[:3, :].permute(1, 0).cpu())\n",
    "            closest_point = closest_point.squeeze()\n",
    "            prediction_cloud = point_prediction[i_batch, :, closest_point]\n",
    "            prediction_batch = torch.cat((prediction_batch, prediction_cloud), 1)\n",
    "            prediction_batches.append(prediction_cloud)\n",
    "        return prediction_batch.permute(1, 0), prediction_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We train the model during `n_epoch` epochs and evaluate it every i-th (`n_epoch_test`) epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def train_full(args, fold_id):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "    # initialize the model\n",
    "    model = PointNet(args.MLP_1, args.MLP_2, args.MLP_3, args)\n",
    "    writer = SummaryWriter(results_path + \"runs/\"+run_name + \"fold_\" + str(fold_id) +\"/\")\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "    print(model)\n",
    "\n",
    "    # define the classifier\n",
    "    PCC = PointCloudClassifier(args)\n",
    "\n",
    "    # define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.lr_decay)\n",
    "\n",
    "    for i_epoch in range(args.n_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "        # train one epoch\n",
    "        train_losses = train(model, PCC, train_set, params, optimizer, args)\n",
    "        writer = write_to_writer(writer, args, i_epoch, train_losses, train=True)\n",
    "\n",
    "        if (i_epoch + 1) % args.n_epoch_test == 0:\n",
    "            if (i_epoch + 1) == args.n_epoch:   # if last epoch, we creare 2D images with points projections\n",
    "                test_losses = eval(model, PCC, test_set, params, args, test_list, mean_dataset, stats_path, stats_file, last_epoch=True)\n",
    "            else:\n",
    "                test_losses = eval(model, PCC, test_set, params, args, test_list, mean_dataset, stats_path, stats_file)\n",
    "            gc.collect()\n",
    "            writer = write_to_writer(writer, args, i_epoch, test_losses, train=False)\n",
    "    writer.flush()\n",
    "\n",
    "    final_train_losses_list = train_losses\n",
    "    final_test_losses_list = test_losses\n",
    "    return model, final_train_losses_list, final_test_losses_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally train the model for `args.fold` folds and do the cross validation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation\n",
      "Cross-validation FOLD = 1\n",
      "Total number of parameters: 25028\n",
      "PointNet(\n",
      "  (MLP_1): Sequential(\n",
      "    (0): Conv1d(9, 32, kernel_size=(1,), stride=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (MLP_2): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (MLP_3): Sequential(\n",
      "    (0): Conv1d(160, 64, kernel_size=(1,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Dropout(p=0.4, inplace=False)\n",
      "    (7): Conv1d(32, 4, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (maxpool): MaxPool1d(kernel_size=4096, stride=4096, padding=0, dilation=1, ceil_mode=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\u001B[100mEpoch   0 -> Train Loss: 1.6301 Train Loss Abs: 0.2833 Train Loss Log: 1.3468\u001B[0m\n",
      "0.015019893646240234\n",
      "POINT_OBS114  Pred [0.32 0.68 0.07 0.15] GT [0.5  0.5  0.75  nan 0.75]\n",
      "0.01479339599609375\n",
      "POINT_OBS21  Pred [0.32 0.68 0.07 0.15] GT [0.9  0.1  0.25  nan 0.9 ]\n",
      "0.011347293853759766\n",
      "POINT_OBS23  Pred [0.32 0.68 0.07 0.15] GT [0.75 0.25 0.25  nan 0.75]\n",
      "0.009492158889770508\n",
      "POINT_OBS28  Pred [0.32 0.68 0.07 0.15] GT [0.75 0.25 0.5   nan 0.81]\n",
      "0.014646530151367188\n",
      "POINT_OBS38  Pred [0.32 0.68 0.07 0.15] GT [0.5 0.5 0.1 nan 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f78551efa50>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 259, in resize\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f78551ef6d0>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f78551ef6d0>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f7854b0e590>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 259, in resize\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f785467b850>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f785467b850>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f783301f550>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 259, in resize\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f7854bf4410>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f7854bf4410>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f7832326ed0>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 259, in resize\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f78331a6490>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f78331a6490>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f782fec4390>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 259, in resize\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f783242a550>\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/_backend_tk.py\", line 270, in idle_draw\n",
      "    self.draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_tkagg.py\", line 9, in draw\n",
      "    super(FigureCanvasTkAgg, self).draw()\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\", line 393, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/ign.fr/ekalinicheva/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\", line 1556, in _draw\n",
      "    def _draw(renderer): raise Done(renderer)\n",
      "matplotlib.backend_bases._get_renderer.<locals>.Done: <matplotlib.backends.backend_agg.RendererAgg object at 0x7f783242a550>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-58-99fcee81c130>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m                                         functools.partial(cloud_loader, dataset=dataset, df_gt=df_gt, train=True, args=args))\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m     \u001B[0mtrained_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_train_losses_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_test_losses_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_full\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfold_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;31m# save the trained model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-57-691500c031ae>\u001B[0m in \u001B[0;36mtrain_full\u001B[0;34m(args, fold_id)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi_epoch\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_epoch_test\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi_epoch\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_epoch\u001B[0m\u001B[0;34m:\u001B[0m   \u001B[0;31m# if last epoch, we creare 2D images with points projections\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m                 \u001B[0mtest_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPCC\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmean_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstats_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstats_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_epoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m                 \u001B[0mtest_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPCC\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmean_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstats_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstats_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-55-03aeeb8b177b>\u001B[0m in \u001B[0;36meval\u001B[0;34m(model, PCC, test_set, params, args, test_list, mean_dataset, stats_path, stats_file, last_epoch)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m             create_final_images(pred_pl, gt, pred_pointwise_b, cloud, likelihood, test_list[index_batch], mean_dataset, stats_path, stats_file,\n\u001B[0;32m---> 72\u001B[0;31m                                 args, adm=pred_adm)  # create final images with stratum values\n\u001B[0m\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/plot_vegetation_coverage/utils/create_final_images.py\u001B[0m in \u001B[0;36mcreate_final_images\u001B[0;34m(pred_pl, gt, pred_pointwise_b, cloud, likelihood, plot_name, mean_dataset, stats_path, stats_file, args, create_raster, adm)\u001B[0m\n\u001B[1;32m    284\u001B[0m                   scores=likelihood, image_high_veg=image_high_veg)\n\u001B[1;32m    285\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnb_stratum\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m             \u001B[0mvisualize_article\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_ground\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage_med_veg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage_high_veg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcurrent_cloud\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstats_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtxt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/plot_vegetation_coverage/utils/create_final_images.py\u001B[0m in \u001B[0;36mvisualize_article\u001B[0;34m(image_soil, image_med_veg, image_high_veg, cloud, pl_id, stats_path, args, txt)\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtxt\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m         \u001B[0mfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m.05\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtxt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'center'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstats_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mpl_id\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_article.svg'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"svg\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbbox_inches\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tight\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/pyplot.py\u001B[0m in \u001B[0;36msavefig\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    721\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m     \u001B[0mfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgcf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 723\u001B[0;31m     \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    724\u001B[0m     \u001B[0mfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcanvas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw_idle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m   \u001B[0;31m# need this if 'transparent=True' to reset colors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    725\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/figure.py\u001B[0m in \u001B[0;36msavefig\u001B[0;34m(self, fname, transparent, **kwargs)\u001B[0m\n\u001B[1;32m   2201\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_visible\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframeon\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2202\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2203\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcanvas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprint_figure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2204\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2205\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mframeon\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001B[0m in \u001B[0;36mprint_figure\u001B[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001B[0m\n\u001B[1;32m   2124\u001B[0m                     \u001B[0morientation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morientation\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2125\u001B[0m                     \u001B[0mbbox_inches_restore\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_bbox_inches_restore\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2126\u001B[0;31m                     **kwargs)\n\u001B[0m\u001B[1;32m   2127\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2128\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mbbox_inches\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mrestore_bbox\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_svg.py\u001B[0m in \u001B[0;36mprint_svg\u001B[0;34m(self, filename, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1194\u001B[0m                 \u001B[0mdetach\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1195\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1196\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_print_svg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1197\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1198\u001B[0m             \u001B[0;31m# Detach underlying stream from wrapper so that it remains open in\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_svg.py\u001B[0m in \u001B[0;36m_print_svg\u001B[0;34m(self, filename, fh, dpi, bbox_inches_restore, **kwargs)\u001B[0m\n\u001B[1;32m   1219\u001B[0m             bbox_inches_restore=bbox_inches_restore)\n\u001B[1;32m   1220\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1221\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1222\u001B[0m         \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1223\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/artist.py\u001B[0m in \u001B[0;36mdraw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0martist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0martist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_agg_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/figure.py\u001B[0m in \u001B[0;36mdraw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m   1734\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1735\u001B[0m             mimage._draw_list_compositing_images(\n\u001B[0;32m-> 1736\u001B[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001B[0m\u001B[1;32m   1737\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m             \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose_group\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'figure'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/image.py\u001B[0m in \u001B[0;36m_draw_list_compositing_images\u001B[0;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[1;32m    135\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnot_composite\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhas_images\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[0;32min\u001B[0m \u001B[0martists\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m             \u001B[0ma\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;31m# Composite any adjacent images together\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/artist.py\u001B[0m in \u001B[0;36mdraw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0martist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0martist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_agg_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py\u001B[0m in \u001B[0;36mdraw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m         \u001B[0;31m# Then rest\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 310\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_axis_position\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/artist.py\u001B[0m in \u001B[0;36mdraw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0martist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0martist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_agg_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36mdraw\u001B[0;34m(self, renderer, inframe)\u001B[0m\n\u001B[1;32m   2628\u001B[0m             \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_rasterizing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2629\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2630\u001B[0;31m         \u001B[0mmimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_draw_list_compositing_images\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0martists\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2631\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2632\u001B[0m         \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose_group\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'axes'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/image.py\u001B[0m in \u001B[0;36m_draw_list_compositing_images\u001B[0;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[1;32m    135\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnot_composite\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhas_images\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[0;32min\u001B[0m \u001B[0martists\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m             \u001B[0ma\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;31m# Composite any adjacent images together\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/artist.py\u001B[0m in \u001B[0;36mdraw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0martist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0martist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_agg_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/collections.py\u001B[0m in \u001B[0;36mdraw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m    892\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    893\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_sizes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sizes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdpi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 894\u001B[0;31m         \u001B[0mCollection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    895\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    896\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/artist.py\u001B[0m in \u001B[0;36mdraw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0mrenderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0martist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrenderer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0martist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_agg_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/collections.py\u001B[0m in \u001B[0;36mdraw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m    367\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_linewidths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_linestyles\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_antialiaseds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_urls\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 369\u001B[0;31m                 self._offset_position)\n\u001B[0m\u001B[1;32m    370\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m         \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrestore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_svg.py\u001B[0m in \u001B[0;36mdraw_path_collection\u001B[0;34m(self, gc, master_transform, paths, all_transforms, offsets, offsetTrans, facecolors, edgecolors, linewidths, linestyles, antialiaseds, urls, offset_position)\u001B[0m\n\u001B[1;32m    642\u001B[0m                 \u001B[0;34m'style'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_style\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgc0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrgbFace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    643\u001B[0m                 }\n\u001B[0;32m--> 644\u001B[0;31m             \u001B[0mwriter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'use'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrib\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrib\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    645\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mclipid\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    646\u001B[0m                 \u001B[0mwriter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'g'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_svg.py\u001B[0m in \u001B[0;36melement\u001B[0;34m(self, tag, text, attrib, **extra)\u001B[0m\n\u001B[1;32m    231\u001B[0m         \u001B[0momitted\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m         \"\"\"\n\u001B[0;32m--> 233\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrib\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mextra\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    234\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/safe_environment/lib/python3.7/site-packages/matplotlib/backends/backend_svg.py\u001B[0m in \u001B[0;36mstart\u001B[0;34m(self, tag, attrib, **extra)\u001B[0m\n\u001B[1;32m    150\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    151\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__tags\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 152\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__indentation\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__tags\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    153\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"<%s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mattrib\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mextra\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df_gt = pd.read_csv(os.path.join(args.path, args.gt_file), sep=',', header=0)  # we open GT file\n",
    "placettes = df_gt['Name'].to_numpy()    # We extract the names of the plots to create train and test list\n",
    "\n",
    "# We use several folds for cross validation (set the number in args)\n",
    "kf = KFold(n_splits=args.folds, random_state=42, shuffle=True)\n",
    "\n",
    "# None lists that will stock stats for each fold, so we can compute the mean at the end\n",
    "all_folds_loss_train_lists = None\n",
    "all_folds_loss_test_lists = None\n",
    "\n",
    "#cross-validation\n",
    "fold_id = 1\n",
    "print(\"Starting cross-validation\")\n",
    "for train_ind, test_ind in kf.split(placettes):\n",
    "    print(\"Cross-validation FOLD = %d\" % (fold_id))\n",
    "    train_list = placettes[train_ind]\n",
    "    test_list = placettes[test_ind]\n",
    "\n",
    "    # generate the train and test dataset\n",
    "    test_set = tnt.dataset.ListDataset(test_list,\n",
    "                                       functools.partial(cloud_loader, dataset=dataset, df_gt=df_gt, train=False, args=args))\n",
    "    train_set = tnt.dataset.ListDataset(train_list,\n",
    "                                        functools.partial(cloud_loader, dataset=dataset, df_gt=df_gt, train=True, args=args))\n",
    "\n",
    "    trained_model, final_train_losses_list, final_test_losses_list = train_full(args, fold_id)\n",
    "\n",
    "    # save the trained model\n",
    "    PATH = os.path.join(stats_path, \"model_ss_\" + str(args.subsample_size) + \"_dp_\" + str(args.diam_pix) + \"_fold_\" + str(fold_id) + \".pt\")\n",
    "    torch.save(trained_model, PATH)\n",
    "\n",
    "    # We compute stats per fold\n",
    "    all_folds_loss_train_lists, all_folds_loss_test_lists = stats_per_fold(all_folds_loss_train_lists, all_folds_loss_test_lists, final_train_losses_list, final_test_losses_list, stats_file, fold_id, args)\n",
    "\n",
    "    print_stats(stats_file,\n",
    "                \"training time \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))),\n",
    "                print_to_console=True)\n",
    "    fold_id += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute mean stats for 5 folds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "stats_for_all_folds(all_folds_loss_train_lists, all_folds_loss_test_lists, stats_file, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}